[package]
name = "auranexus"
version = "2.0.0"
description = "AuraNexus - HIPAA-Compliant AI Companion (Native Rust)"
authors = ["AuraNexus Team"]
license = ""
repository = ""
edition = "2021"

[build-dependencies]
tauri-build = { version = "1.5", features = [] }

[dependencies]
# Tauri core
tauri = { version = "1.5", features = ["api-all"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Async runtime
tokio = { version = "1", features = ["full"] }

# LLM inference (native Rust, CPU-only for now)
llama-cpp-2 = { version = "0.1", features = [] }
# NOTE: GPU support (CUDA/Vulkan) has build issues on Windows - CPU works reliably

# Vector database (embedded) - We'll implement embeddings separately
# lancedb = "0.5"  # Temporarily disabled due to protobuf complexity

# Additional utilities
anyhow = "1.0"
walkdir = "2.4"  # Recursive directory scanning for model files
reqwest = { version = "0.11", features = ["stream", "json", "blocking"] }  # HTTP downloads and LLM server calls
futures-util = "0.3"  # Async streaming utilities
chrono = { version = "0.4", features = ["serde"] }  # Timestamps for file metadata
dirs = "5.0"  # Get user directories (home, etc.)
thiserror = "1.0"
uuid = { version = "1.0", features = ["v4", "serde"] }
parking_lot = "0.12"
regex = "1.10"  # For text chunking sentence detection

# Python interop - Connect to existing Python backend
pyo3 = { version = "0.20", features = ["auto-initialize"] }

[features]
default = []
custom-protocol = ["tauri/custom-protocol"]
